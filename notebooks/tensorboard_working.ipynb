{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6B95Hb6YVgPZ"
      },
      "outputs": [],
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_wqSAZExy6xV"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ao7fJW1Pyiza"
      },
      "outputs": [],
      "source": [
        "# Clear any logs from previous runs\n",
        "!rm -rf ./logs/ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and preprocess data\n",
        "data = pd.read_csv('MERGED_BTC-15m-2015-07-20-to-2024-06-27.csv', parse_dates=True, index_col='datetime')\n",
        "\n",
        "# Calculate the index for splitting into 75% training and 25% testing\n",
        "# split_index is the index position of the end of the first 75% of the data\n",
        "split_index = int(len(data) * 0.75)\n",
        "# print(split_index)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "# grabs all the data up to the index from split index (first 75%)\n",
        "data = data.iloc[:split_index]\n",
        "data = data.dropna()\n",
        "prices = data['close'].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/j3/f2pn80cx63z808kw83_jxw3w0000gn/T/ipykernel_40851/94713225.py:6: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  y = prices[i + window_size]\n"
          ]
        }
      ],
      "source": [
        "# Create the dataset with windowing method\n",
        "def create_dataset(prices, window_size):\n",
        "    data = []\n",
        "    for i in range(len(prices) - window_size):\n",
        "        X = prices[i:i + window_size]\n",
        "        y = prices[i + window_size]\n",
        "        data.append(list(X) + [y])\n",
        "    columns = [f't-{i}' for i in range(window_size, 0, -1)] + ['t']\n",
        "    return pd.DataFrame(data, columns=columns)\n",
        "\n",
        "window_size = 40\n",
        "dataset = create_dataset(prices, window_size)\n",
        "# print(dataset)\n",
        "X = dataset.iloc[:, :-1]\n",
        "y = dataset.iloc[:, -1]\n",
        "# print(X)\n",
        "# print(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Normalize features and target using the training data\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "y_train = y_train.values.reshape(-1, 1)\n",
        "y_test = y_test.values.reshape(-1, 1)\n",
        "y_scaler = MinMaxScaler()\n",
        "y_scaler.fit(y_train)\n",
        "y_train_scaled = y_scaler.transform(y_train)\n",
        "y_test_scaled = y_scaler.transform(y_test)\n",
        "\n",
        "# sanity check\n",
        "# print(window_size == X_train.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/dev/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Dense(10, input_dim=window_size, activation='relu'))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m5499/5499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 430us/step - loss: 0.0684 - mean_absolute_percentage_error: 3299.8037 - val_loss: 0.0551 - val_mean_absolute_percentage_error: 2310.5474\n",
            "Epoch 2/10\n",
            "\u001b[1m5499/5499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 405us/step - loss: 0.0528 - mean_absolute_percentage_error: 3058.1353 - val_loss: 0.0413 - val_mean_absolute_percentage_error: 2003.7570\n",
            "Epoch 3/10\n",
            "\u001b[1m5499/5499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 470us/step - loss: 0.0319 - mean_absolute_percentage_error: 2567.0957 - val_loss: 0.0025 - val_mean_absolute_percentage_error: 622.9353\n",
            "Epoch 4/10\n",
            "\u001b[1m5499/5499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 460us/step - loss: 0.0010 - mean_absolute_percentage_error: 450.5447 - val_loss: 1.1337e-04 - val_mean_absolute_percentage_error: 125.0505\n",
            "Epoch 5/10\n",
            "\u001b[1m5499/5499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 521us/step - loss: 9.9504e-05 - mean_absolute_percentage_error: 297.9168 - val_loss: 7.0149e-05 - val_mean_absolute_percentage_error: 92.4844\n",
            "Epoch 6/10\n",
            "\u001b[1m5499/5499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 456us/step - loss: 6.4833e-05 - mean_absolute_percentage_error: 147.9149 - val_loss: 5.2842e-05 - val_mean_absolute_percentage_error: 73.8357\n",
            "Epoch 7/10\n",
            "\u001b[1m5499/5499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 440us/step - loss: 4.9257e-05 - mean_absolute_percentage_error: 93.4983 - val_loss: 4.2985e-05 - val_mean_absolute_percentage_error: 57.7860\n",
            "Epoch 8/10\n",
            "\u001b[1m5499/5499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 421us/step - loss: 4.1484e-05 - mean_absolute_percentage_error: 94.7011 - val_loss: 3.7665e-05 - val_mean_absolute_percentage_error: 47.2942\n",
            "Epoch 9/10\n",
            "\u001b[1m5499/5499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 450us/step - loss: 3.6008e-05 - mean_absolute_percentage_error: 139.2350 - val_loss: 3.4788e-05 - val_mean_absolute_percentage_error: 42.2559\n",
            "Epoch 10/10\n",
            "\u001b[1m5499/5499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 431us/step - loss: 3.3507e-05 - mean_absolute_percentage_error: 45.6145 - val_loss: 3.2974e-05 - val_mean_absolute_percentage_error: 39.5515\n"
          ]
        }
      ],
      "source": [
        "# Compile the model with a lower learning rate\n",
        "model.compile(optimizer=SGD(learning_rate=0.001), loss='mean_squared_error', metrics=[tf.keras.metrics.MeanAbsolutePercentageError()])\n",
        "\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_scaled, y_train_scaled, epochs=10, batch_size=32, validation_data=(X_test_scaled, y_test_scaled), verbose=1, callbacks=[tensorboard_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKUjdIoV87um"
      },
      "source": [
        "## Using TensorBoard with Keras Model.fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CL_lxdn8-Sv"
      },
      "source": [
        "When training with Keras's [Model.fit()](https://www.tensorflow.org/api_docs/python/tf/keras/models/Model#fit), adding the `tf.keras.callbacks.TensorBoard` callback ensures that logs are created and stored. Additionally, enable histogram computation every epoch with `histogram_freq=1` (this is off by default)\n",
        "\n",
        "Place the logs in a timestamped subdirectory to allow easy selection of different training runs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asjGpmD09dRl"
      },
      "source": [
        "Start TensorBoard through the command line or within a notebook experience. The two interfaces are generally the same. In notebooks, use the `%tensorboard` line magic. On the command line, run the same command without \"%\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "A4UKgTLb9fKI"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-129fdfd7ac41ca33\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-129fdfd7ac41ca33\");\n",
              "          const url = new URL(\"/\", window.location);\n",
              "          const port = 6006;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%tensorboard --logdir logs/fit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCsoUNb6YhGc"
      },
      "source": [
        "<!-- <img class=\"tfo-display-only-on-site\" src=\"https://github.com/tensorflow/tensorboard/blob/master/docs/images/quickstart_model_fit.png?raw=1\"/> -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gi4PaRm39of2"
      },
      "source": [
        "A brief overview of the visualizations created in this example and the dashboards (tabs in top navigation bar) where they can be found:\n",
        "\n",
        "* **Scalars** show how the loss and metrics change with every epoch. You can use them to also track training speed, learning rate, and other scalar values. Scalars can be found in the **Time Series** or **Scalars** dashboards.\n",
        "* **Graphs** help you visualize your model. In this case, the Keras graph of layers is shown which can help you ensure it is built correctly. Graphs can be found in the **Graphs** dashboard.\n",
        "* **Histograms** and **Distributions** show the distribution of a Tensor over time. This can be useful to visualize weights and biases and verify that they are changing in an expected way. Histograms can be found in the **Time Series** or **Histograms** dashboards. Distributions can be found in the **Distributions** dashboard.\n",
        "\n",
        "Additional TensorBoard dashboards are automatically enabled when you log other types of data. For example, the Keras TensorBoard callback lets you log images and embeddings as well. You can see what other dashboards are available in TensorBoard by clicking on the \"inactive\" dropdown towards the top right."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "get_started.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
